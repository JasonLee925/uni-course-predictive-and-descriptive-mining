{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üëâüèª Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   userId     8000 non-null   int64  \n",
      " 1   movieId    7958 non-null   float64\n",
      " 2   rating     8000 non-null   float64\n",
      " 3   timestamp  8000 non-null   object \n",
      " 4   imdbId     7759 non-null   object \n",
      " 5   title      8000 non-null   object \n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 375.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('D1.csv')\n",
    "print(df.info())\n",
    "# print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-1\n",
    "What pre-processing was required on the dataset before building the association mining\n",
    "model? What variables did you include in the analysis? Justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movieId: \n",
    "# dropping Nan values first\n",
    "df = df.dropna(subset=['movieId'])\n",
    "\n",
    "# float -> int \n",
    "df[\"movieId\"] = df[\"movieId\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdbId: \n",
    "# dropping Nan values first\n",
    "df = df.dropna(subset=['imdbId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7726 entries, 0 to 7999\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   userId     7726 non-null   int64  \n",
      " 1   movieId    7726 non-null   int64  \n",
      " 2   rating     7726 non-null   float64\n",
      " 3   timestamp  7726 non-null   object \n",
      " 4   imdbId     7726 non-null   object \n",
      " 5   title      7726 non-null   object \n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 422.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() # for checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-2\n",
    "Conduct association mining and answer the following:\n",
    "\n",
    "- a. What ‚Äòmin_support‚Äô and `min_confidence‚Äô thresholds were set for this mining\n",
    "exercise? Rationalize why these values were chosen.\n",
    "- b. Report the top-5 (interesting) rules and interpret them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚åó task-1-2-a. ‚Äòmin_support‚Äô and `min_confidence‚Äô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set these two values to defualt for now cause I have no clue üòµ‚Äçüí´\n",
    "\n",
    "- `min_support` = 0.1\n",
    "- `min_confidence` = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚åó task-1-2-b. top-5 (interesting) rules and interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId\n",
      "2    [Interview with the Vampire, Ed Wood, Disclosu...\n",
      "3    [The White Stripes: Under Great White Northern...\n",
      "4    [Addams Family Values, Mystery Men, The Fly, F...\n",
      "5    [Road to Perdition, Almost Famous, Hotel Rwand...\n",
      "6                                       [Spider-Man 2]\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# generating the \n",
    "transactions = df.groupby(['userId'])['title'].apply(list)\n",
    "print(transactions.head(5)) # this output get cut\n",
    "# for t in transactions[:5]:\n",
    "#     print(sorted(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Left_side, Right_side, Support, Confidence, Lift]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Apriori algorithm\n",
    "from apyori import apriori\n",
    "transaction_list = list(transactions)\n",
    "results = list(apriori(transaction_list))\n",
    "\n",
    "def convert_apriori_results_to_pandas_df(results):\n",
    "    rules = []\n",
    "    for rule_set in results:\n",
    "        for rule in rule_set.ordered_statistics:\n",
    "            rules.append([','.join(rule.items_base), ','.join(rule.items_add),\n",
    "                    rule_set.support, rule.confidence, rule.lift])\n",
    "    # typecast it to pandas df\n",
    "    return pd.DataFrame(rules, columns=['Left_side', 'Right_side', 'Support', 'Confidence', 'Lift'])\n",
    "\n",
    "result_df = convert_apriori_results_to_pandas_df(results)\n",
    "result_df_sorted = result_df.sort_values(by='Lift', ascending=False)\n",
    "print(result_df_sorted.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, didnt get any shit ü•¥\n",
    "\n",
    "So now, we might try to decrease the value of `min_support` to see if there is any notable rules appearing. <br/>\n",
    "Why? Beacuse `min_support` acts like a threshold for the algorithm to determine which rules should be selected, and the selection criterion is based on the the probability of both A and B apprearing together (e.g. A=>B). <br/>\n",
    "If the probability is higher than the minimum threshold, the rule is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Left_side             Right_side   Support  Confidence  Lift\n",
      "0              2001: A Space Odyssey  0.012442    0.012442   1.0\n",
      "173                            Speed  0.023328    0.023328   1.0\n",
      "160               Return of the Jedi  0.034215    0.034215   1.0\n",
      "161                            Rocky  0.012442    0.012442   1.0\n",
      "162              Saving Private Ryan  0.021773    0.021773   1.0\n",
      "163                 Schindler's List  0.026439    0.026439   1.0\n",
      "164                           Scream  0.010886    0.010886   1.0\n",
      "165                            Se7en  0.027994    0.027994   1.0\n",
      "166            Sense and Sensibility  0.013997    0.013997   1.0\n",
      "167              Shakespeare in Love  0.015552    0.015552   1.0\n"
     ]
    }
   ],
   "source": [
    "# set `min_support` to 0.01\n",
    "\n",
    "results = list(apriori(transaction_list, min_support=0.01))\n",
    "result_df = convert_apriori_results_to_pandas_df(results)\n",
    "# sort\n",
    "result_df_sorted = result_df.sort_values(by='Lift', ascending=False) \n",
    "print(result_df_sorted.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go. <br/>\n",
    "\n",
    "However, the result does not show any valuable insight beacuse the greatest value of \"lift\" = 1. This means the rules have zero correlation, indicating the two itemsets are independent.\n",
    "\n",
    "Let's decrease `min_support` a bit more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Left_side                       Right_side  \\\n",
      "655            Rebel Without a Cause                      Rear Window   \n",
      "654                      Rear Window            Rebel Without a Cause   \n",
      "624       Ace Ventura: Pet Detective                        The Mummy   \n",
      "625                        The Mummy       Ace Ventura: Pet Detective   \n",
      "646                   Twelve Monkeys           Mission: Impossible II   \n",
      "645           Mission: Impossible II                   Twelve Monkeys   \n",
      "630                        Apollo 13           The American President   \n",
      "649                   V for Vendetta  Monty Python and the Holy Grail   \n",
      "648  Monty Python and the Holy Grail                   V for Vendetta   \n",
      "631           The American President                        Apollo 13   \n",
      "\n",
      "      Support  Confidence       Lift  \n",
      "655  0.006221    0.500000  32.150000  \n",
      "654  0.006221    0.400000  32.150000  \n",
      "624  0.006221    0.250000  20.093750  \n",
      "625  0.006221    0.500000  20.093750  \n",
      "646  0.006221    0.266667  15.587879  \n",
      "645  0.006221    0.363636  15.587879  \n",
      "630  0.006221    0.190476  12.247619  \n",
      "649  0.006221    0.266667  12.247619  \n",
      "648  0.006221    0.285714  12.247619  \n",
      "631  0.006221    0.400000  12.247619  \n"
     ]
    }
   ],
   "source": [
    "# set `min_support` to 0.006\n",
    "\n",
    "results = list(apriori(transaction_list, min_support=0.006))\n",
    "result_df = convert_apriori_results_to_pandas_df(results)\n",
    "# sort\n",
    "result_df_sorted = result_df.sort_values(by='Lift', ascending=False) \n",
    "print(result_df_sorted.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Interpretation:** <br/>\n",
    "\n",
    "The result shows high lift in the generated rules, indicating the correlations are high in between the itemsets. <br/>\n",
    "Taking the first row as an example (lift = 32.15):  <br/>\n",
    "When a person watches the movie 'Rebel Without a Cause', they are 32 times more likely to also watch the movie 'Rear Window'.\n",
    "\n",
    "However, since `min_support` is set to a low value (=0.006), this means the generated rules are considered rare, indicating the relationship between the itemsets is sparse. <br/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-3\n",
    "List five most watched movies by individuals who have also watched ‚ÄòThe Shawshank Redemption‚Äô movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left_side</th>\n",
       "      <th>Right_side</th>\n",
       "      <th>Support</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td></td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>0.032659</td>\n",
       "      <td>0.032659</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Left_side                Right_side   Support  Confidence  Lift\n",
       "551            The Shawshank Redemption  0.032659    0.032659   1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_sorted.loc[(result_df_sorted['Left_side'].isin(['The Shawshank Redemption'])) | (result_df_sorted['Right_side'].isin(['The Shawshank Redemption']))].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no movie that is found to be watched when The Shawshank Redemption is also watch, because the result is probably filtered out by the `min_support`.\n",
    "\n",
    "Although I dont see the point of finding the answer for the quesiton,  the code below can calculate the most watched movies base on the transactions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *- Hide below -*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five most watched movies by individuals who have also watched 'The Shawshank Redemption':\n",
      "Jurassic Park: 3 times\n",
      "The Client: 3 times\n",
      "Stargate: 3 times\n",
      "The Lord of the Rings: The Fellowship of the Ring: 3 times\n",
      "Citizen Kane: 2 times\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "lists_of_movies = []\n",
    "for t in transactions:\n",
    "    if 'The Shawshank Redemption' in t:\n",
    "        lists_of_movies.append(t)\n",
    "        \n",
    "\n",
    "# Flatten the lists of movies\n",
    "all_movies = [movie for sublist in lists_of_movies for movie in sublist]\n",
    "\n",
    "# Count the occurrences of each movie\n",
    "movie_counter = Counter(all_movies)\n",
    "\n",
    "# Remove 'The Shawshank Redemption' from the movie counter\n",
    "del movie_counter['The Shawshank Redemption']\n",
    "\n",
    "# Filter movies watched by individuals who have watched 'The Shawshank Redemption'\n",
    "filtered_movies = {movie: count for movie, count in movie_counter.items() if movie in all_movies}\n",
    "\n",
    "# Sort movies by the number of times they were watched\n",
    "sorted_movies = sorted(filtered_movies.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top five most watched movies\n",
    "top_five_movies = sorted_movies[:5]\n",
    "\n",
    "# Print the result\n",
    "print(\"Five most watched movies by individuals who have also watched 'The Shawshank Redemption':\")\n",
    "for movie, count in top_five_movies:\n",
    "    print(f\"{movie}: {count} times\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *- hide above -*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-4\n",
    "Can you perform sequence analysis on this dataset? If yes, present your results. If not,\n",
    "rationalize why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As aforementioned, since the relationship between data is quite sparse, it might not be recommended to apply sequence analysis to this dataset as it will only result in sparser relationship, making it harder to find more insights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-5\n",
    "In what ways can the results of this task be utilized by the relevant decision-makers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show high correlations (high lift) between data in a small number of cases (low support), meaning that there are some ways we can utitlise this result to make decision: \n",
    "- Improve the niche market by promoting the movies that are highly correlated together.\n",
    "- Some movies are found very old (made in 1980-2000). Ôº∑e may aim to target the market.\n",
    "- Make a decision on taking more variables (e.g. rating or genre if it's possible) into account so that we can create more insight from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
